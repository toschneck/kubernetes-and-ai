
#
persistence:
  models:
    enabled: true
    annotations: {}
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 25Gi
    globalMount: /models
  output:
    enabled: true
    annotations: {}
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 5Gi
    globalMount: /tmp/generated

deployment:
  image:
    tag: v2.17.0-aio-gpu-nvidia-cuda-12
    #tag: v2.15.0-aio-gpu-nvidia-cuda-12


service:
  # type: LoadBalancer
  # If deferring to an internal only load balancer
  # externalTrafficPolicy: Local
  port: 80
  #annotations:
  #  "external-dns.alpha.kubernetes.io/hostname": "localai-kcd.demo.kubermatic.io."
  # If using an AWS load balancer, you'll need to override the default 60s load balancer idle timeout
  # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "1200"

ingress:
  enabled: true
  className: "nginx"
  annotations:
    #kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod-nginx"
    #external-dns.alpha.kubernetes.io/hostname: localai-cl.demo.kubermatic.io
  hosts:
    - host: localai.cloudland.demo.kubermatic.io
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls:
    - secretName: local-ai-tls
      hosts:
      - localai.cloudland.demo.kubermatic.io

nodeSelector:
  type: gpu

tolerations:
  - key: workload
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"