
#
persistence:
  models:
    enabled: true
    annotations: {}
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 25Gi
    globalMount: /models
  output:
    enabled: true
    annotations: {}
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 5Gi
    globalMount: /tmp/generated

deployment:
  image:
    tag: v2.15.0-aio-gpu-nvidia-cuda-12


service:
  # type: LoadBalancer
  # If deferring to an internal only load balancer
  # externalTrafficPolicy: Local
  port: 80
  #annotations:
  #  "external-dns.alpha.kubernetes.io/hostname": "localai-kcd.demo.kubermatic.io."
  # If using an AWS load balancer, you'll need to override the default 60s load balancer idle timeout
  # service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "1200"

ingress:
  enabled: false
  className: "istio"
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: localai-kcd.demo.kubermatic.io
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

nodeSelector:
  type: gpu

tolerations:
  - key: workload
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"